# 线性回归/分类(应该也可以分类吧?)

- 线性回归的模型既然是线性的那么大概会是这样的:  
<!--这里本来有张图的但是markdown放不了图所以没有-->  
由样本训练出来的预测曲线 $F$ 在投入未知样本 $X_i$ 后可由 $F(X_i)$ 预测 $X_i$ 的标签 $Y_i$ :  

```math
\hat{Y} = W \cdot X + B

```

机器显然并不知道真实的 $W$ 和 $B$ 是多少,它只有我们默认给出的 $W_0$ 和 $B_0$ ,  

以及样本 $X$ { $X_1$ , $X_2$ …… $X_n$ }和其对应样本 $Y$ { $Y_1$ , $Y_2$ …… $Y_n$ },那么如何得出真正的 $W$ 和 $B$ 呢  

- 顺带一提, 用于分类的话答案是可以, 但通常不直接使用. 简单的二元分类可以通过一个阈值函数( 如单位阶跃函数、Sigmoid) 将线性回归的输出 $W \cdot X + B$ 映射到 0/1 类别, 但这通常不如专门的分类模型 (如逻辑回归) 稳健, 这里简单备注一句.

---

## 损失函数

```math
L = \frac{1}{2m} \sum_{i=1}^{m}(Y_i - \hat{Y}_i)^2 = \frac{1}{2m} \sum_{i=1}^{m}(Y_i - (W \cdot X_i + B))^2
```  

其中:

$Y_i$ 是真实标签,  
$\hat{Y}_i$ 是预测标签, 即 $f(x)$ (就是$W \cdot X_i + B$) ,  
$m$ 是样本数量

- 目前说我其实看不懂这个公式是如何推导的, 但是我们只需要知道它是如何运作的就可以了, 不是吗?(面向对象说是)

- 在python里一般这么写:

```python
def compute_loss(X_train, y_train, W, B):
    """
    计算均方误差损失
    
    参数:
    X_train: 特征矩阵
    y_train: 真实标签
    W: 权重
    B: 偏置
    
    返回:
    loss: 平均损失值
    """
    m = len(X_train)    #样本数量m
    total_error = 0
    
    for i in range(m):
        prediction = W * X_train[i] + B  # 线性预测
        error = prediction - y_train[i]
        total_error += error ** 2
        
    loss = total_error / (2 * m)
    return loss
```

- 损失函数的作用:  

1. 衡量模型性能：量化预测值与真实值的差距  
2. 指导参数优化：为梯度下降提供方向  
3. 模型选择依据：比较不同模型的优劣  

## 损失函数（稍微认真一点的版本）

好了，我们知道目标是找到一条直线来拟合数据，但“好”和“坏”总得有个量化的标准。损失函数就是这个“裁判”。

### 公式再审视与变量名解说

```math
L = \frac{1}{2m} \sum_{i=1}^{m}(Y_i - (W \cdot X_i + B))^2
```

让我们像点名单一样，正式认识一下公式里的每一位成员：

- $L$：**损失（Loss）**。我们的核心指标，一个数字。这个数字越大，说明模型当前预测得越离谱；越小则说明预测得越准。我们的终极目标就是把它弄到最小。
- $m$：**样本数量**。就是你有多少个训练数据。加和之后除以 $m$ 是为了求“平均”误差，这样模型不会因为数据集大小不同而得到天差地别的损失值，显得比较专业和稳定。
- $i$：**索引**。一个朴素的循环变量，代表我们正在看第 $i$ 个样本。
- $Y_i$：**第 $i$ 个样本的真实标签**。也就是标准答案。它是已知的、神圣不可侵犯的 Ground Truth。
- $X_i$：**第 $i$ 个样本的特征值**。就是输入数据。
- $W$ 和 $B$：**模型的权重和偏置**。这就是我们要求解的、唯二不知道的两个参数。它们是导致损失 $L$ 产生的“罪魁祸首”，也是我们接下来要优化的“犯人”。
- $(W \cdot X_i + B)$：**模型对第 $i$ 个样本的预测值**。也可以写成 $\hat{Y_i}$。这是模型给出的答案，目前看来基本是错的。
- $(Y_i - (W \cdot X_i + B))$：**第 $i$ 个样本的预测误差**。简单粗暴地用“真实值”减去“预测值”。如果预测高了，误差为负；预测低了，误差为正。这很符合直觉：我比你强，分数就是正的。
- $( ... )^2$：**对误差进行平方**。这是一个神来之笔，主要有两个现实目的：

    1. **消除正负号影响**。不然预测高了和低了误差会相互抵消，模型会天真地以为“平均来看我预测得挺准”。
    2. **放大大的误差**。让模型对那些错得特别离谱的点产生“恐惧”，从而更积极地去修正。

- $\frac{1}{2m}$：**求平均，并附赠一个 $\frac{1}{2}$**。前面的 $\frac{1}{m}$ 是为了求平均误差。多出来的 $\frac{1}{2}$ 是一个纯数学技巧，是为了之后对 $W$ 和 $B$ 求偏导数时，能和平方项产生的 $2$ 约掉，使得求导后的形式非常干净，没有多余的系数。属于是为了方便后续计算而提前做的“格式化”。

### “推导”的直觉（非严格证明）

你问这个公式是怎么推导出来的？坦白说，它更像是“设计”出来的，而不是“推导”出来的。我们基于一些非常合理的需求来构建它：

1. **需求一**：我们需要一个能衡量“预测值”和“真实值”差距的函数。
    - 最直接的想法：差距 = $Y_i - \hat{Y_i}$。

2. **需求二**：这个差距应该总是正数，不能相互抵消。
    - 解决方案一：取绝对值 $|Y_i - \hat{Y_i}|$。这完全可行，得到的叫做“平均绝对误差（MAE）”。
    - 解决方案二：取平方 $(Y_i - \hat{Y_i})^2$。这就是我们正在用的“均方误差（MSE）”。

3. **为什么选择了平方而不是绝对值？**
    - **工程理由**：平方函数处处可导，光滑又漂亮。而绝对值函数在零点处有个尖，不可导，对于依赖求导来寻找最低点的梯度下降法来说，是个麻烦分子。
    - **数学理由**：当误差服从高斯分布（正态分布）时，最小化均方误差等价于在最大似然估计下找到最可能的模型参数。当然，这是后话了，现在你可以简单地理解为“平方在数学上更好处理”。

所以，这个损失函数公式，可以看作是为了满足“衡量差距”、“处理正负号”、“便于优化”这几个核心需求，而设计出的一个**在数学上非常 Convenient** 的方案。

### Python实现（注释版）

```python
def compute_loss(X_train, y_train, W, B):
    """
    计算均方误差损失。一个计算L值的无情机器。

    参数:
    X_train: 特征矩阵，形状通常为(m,)，就是一列数。
    y_train: 真实标签，形状也为(m,)，另一列数。
    W: 当前的权重参数，一个浮点数，模型的身家性命之一。
    B: 当前的偏置参数，一个浮点数，模型的身家性命之二。

    返回:
    loss: 一个浮点数，告诉我们当前模型有多“失败”。
    """
    m = len(X_train)   # 样本数量，用于后续求平均
    total_error = 0     # 总误差，初始化为0，准备开始累加

    # 开始遍历每一个样本，进行审判
    for i in range(m):
        prediction = W * X_train[i] + B  # 用当前的W和B做出预测
        error = prediction - y_train[i]  # 计算单个样本的误差 (注意这里顺序和公式里相反，但平方后没区别)
        total_error += error ** 2        # 将平方误差加入总误差

    # 计算平均误差，并附赠除以2
    loss = total_error / (2 * m)
    return loss
```

### 总结一下，损失函数的作用就是

1. **提供一个明确的优化目标**：让 $L$ 最小。
2. **将模型好坏量化**：把“预测得不错”这种感觉，变成一个可以比较大小的具体数字。
3. **为梯度下降提供路面信息**：通过计算 $L$ 对 $W$ 和 $B$ 的偏导数，告诉我们该往哪个方向调整参数才能让 $L$ 下降。

现在，变量名和作用都清晰了，我们可以心无旁骛地开始研究如何通过“梯度下降”来折磨 $W$ 和 $B$，直到它们让 $L$ 足够小为止。

## 梯度下降

终于来讲如何计算真正的 $W$ 和 $B$ 了

### 导数

在理解梯度下降之前需要理解偏导数以此来计算损失函数对 $W$ 和 $B$ 的变化率
而偏导数的理解需要先理解导数

- 从"平均变化率"逼近到"瞬时变化率"的思想入手, 有:

    对于函数

    $$y = f(x)$$

    设**自变量** $x$ 从 $X_0$ 变化到 $X_0 + \Delta x$ ;

    相应的, **函数值** $y$ 就从 $f(X_0)$ 变化到 $f(X_0 + \Delta x)$.

    这时, $x$ 的变化量为 $\Delta x$ , 函数值的变化量为

    $$\Delta y = f(X_0 + \Delta x) - f(X_0)$$

    我们把这两个量 $\frac{\Delta y}{\Delta x}$ 的比值

    $$\frac{\Delta y}{\Delta x} = \frac{f(X_0 + \Delta x) - f(X_0)}{\Delta x}$$

    叫做函数 $y = f(x)$ 从 $X_0$ 到 $X_0 + \Delta x$ 的**平均变化率**.

    如果当 $\Delta x$ 无限趋近于 0 时, 平均变化率 $\frac{\Delta y}{\Delta x}$ 会无限趋近于一个确定的值, 即 $\frac{\Delta y}{\Delta x}$ 有极限.

    则称$y=f(x)$ 在 $x= X_0$ 处**可导**, 并把这个确定的极限值叫做

    $y = f(x)$ 在 $x = X_0$ 处的**导数**,记作 $f'(X_0)$.

    记作 $f'(X_0)$ 或 $\frac{dy}{dx}|_{(x=X_0)}$ 或 $ Df(X_0)$

    即

    $$f'(X_0) = \lim_{\Delta x \to 0}\frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0} \frac{f(X_0 + \Delta x) - f(X_0)}{\Delta x}$$

## 偏导数

### 从导数到偏导数

在损失函数

```math
L = \frac{1}{2m} \sum_{i=1}^{m}(Y_i - (W \cdot X_i + B))^2
```

中，**L** 同时受到 ***W*** 和 ***B*** 两个参数的影响。  
这就好比一个多元函数：

```math
L = f(W, B)
```

我们想知道：  

- 当 ***W*** 发生微小变化时，**L** 会如何变化?  
- 当 ***B*** 发生微小变化时，**L** 又会如何变化?

这就是**偏导数**要解决的问题：**研究多元函数中，某个单独变量的变化对函数值的影响**。

### 偏导数的定义

对于二元函数 **z = f(x, y)**：

- **对 x 的偏导数**（固定 y 不变）：

```math
\frac{\partial z}{\partial x} = \lim_{\Delta x \to 0} \frac{f(x + \Delta x, y) - f(x, y)}{\Delta x}
```

- **对 y 的偏导数**（固定 x 不变）：

```math
\frac{\partial z}{\partial y} = \lim_{\Delta y \to 0} \frac{f(x, y + \Delta y) - f(x, y)}{\Delta y}
```

那个弯弯的符号 **∂** 就是偏导数的专用符号，读作"偏"。

### 在线性回归中的应用

对于我们的损失函数：

```math
L(W, B) = \frac{1}{2m} \sum_{i=1}^{m}(Y_i - (W \cdot X_i + B))^2
```

我们需要求出 **L** 对 ***W*** 和 ***B*** 的偏导数：

#### 1. 对权重 W 的偏导数

```math
\frac{\partial L}{\partial W} = \frac{1}{m} \sum_{i=1}^{m} [(W \cdot X_i + B) - Y_i] \cdot X_i
```

#### 2. 对偏置 B 的偏导数

```math
\frac{\partial L}{\partial B} = \frac{1}{m} \sum_{i=1}^{m} [(W \cdot X_i + B) - Y_i]
```

### Python实现偏导数计算

```python
def compute_gradients(X_train, y_train, W, B):
    """
    计算损失函数对W和B的偏导数（梯度）
    
    参数:
    X_train: 特征矩阵
    y_train: 真实标签
    W: 当前权重
    B: 当前偏置
    
    返回:
    dW: 损失函数对W的偏导数
    dB: 损失函数对B的偏导数
    """
    m = len(X_train)
    dW = 0
    dB = 0
    
    for i in range(m):
        prediction = W * X_train[i] + B
        error = prediction - y_train[i]
        
        dW += error * X_train[i]  # ∂L/∂W 的累加
        dB += error               # ∂L/∂B 的累加
    
    dW = dW / m  # 求平均值
    dB = dB / m  # 求平均值
    
    return dW, dB
```

### 偏导数的几何意义

如果把损失函数 $L(W, B)$ 想象成一座山：

- $\frac{\partial L}{\partial W}$ 表示：在当前位置，沿 **W** 方向上山/下山的**坡度**
- $\frac{\partial L}{\partial B}$ 表示：在当前位置，沿 **B** 方向上山/下山的**坡度**

这两个偏导数共同告诉我们：哪个方向是"下山"最快的方向——这就是**梯度**的概念.

## 梯度下降法

有了偏导数, 我们就可以使用**梯度下降法**来优化我们的参数 ***W*** 和 ***B*** 了.

## 参数更新公式

### 基本更新规则

```math
W_{\text{new}} = W_{\text{old}} - \alpha \cdot \frac{\partial L}{\partial W}
```

```math
B_{\text{new}} = B_{\text{old}} - \alpha \cdot \frac{\partial L}{\partial B}
```

其中：

- $\alpha$ 是**学习率**（learning rate）
- $\frac{\partial L}{\partial W}$ 和 $\frac{\partial L}{\partial B}$ 是梯度

### 代入具体公式

```math
W_{\text{new}} = W_{\text{old}} - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (\hat{Y}_i - Y_i) \cdot X_i
```

```math
B_{\text{new}} = B_{\text{old}} - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (\hat{Y}_i - Y_i)
```

## 为什么这样更新？

### 直观理解

**想象你在山坡上**：

- 梯度 $\frac{\partial L}{\partial W}$ 告诉你山坡在W方向的陡峭程度
- 如果梯度是**正数**：W增加会导致损失增加 → 应该**减小**W
- 如果梯度是**负数**：W增加会导致损失减少 → 应该**增加**W

**学习率的作用**：

- 学习率太大：可能"跨过"最低点，甚至发散
- 学习率太小：收敛太慢，需要很多步才能到达最低点

## 完整更新过程代码

```python
def update_parameters(X, y, W, B, learning_rate=0.01):
    """
    一次完整的参数更新过程
    """
    m = len(X)
    
    # 1. 计算预测值
    predictions = W * X + B
    
    # 2. 计算梯度
    dW = 0
    dB = 0
    
    for i in range(m):
        error = predictions[i] - y[i]  # 预测值 - 真实值
        dW += error * X[i]  # W的梯度
        dB += error         # B的梯度
    
    # 3. 求平均梯度
    dW = dW / m
    dB = dB / m
    
    # 4. 更新参数
    W_new = W - learning_rate * dW
    B_new = B - learning_rate * dB
    
    return W_new, B_new
```

## 具体例子演示

假设我们有3个数据点：

```python
X = [1, 2, 3]  # 特征
y = [2, 4, 6]  # 真实值（完美符合 y = 2x）
```

**第一次迭代**：

- 初始值：W=0, B=0, 学习率=0.1
- 预测值：[0, 0, 0]
- 误差：[-2, -4, -6]
- 梯度计算：
  - ${\partial W} = (-2×1 + -4×2 + -6×3)/3 = -28/3 ≈ -9.33$
  - ${\partial B} = (-2 + -4 + -6)/3 = -12/3 = -4$
- 参数更新：
  - $W_{\text{new}} = 0 - 0.1 × (-9.33) = 0.933$
  - $B_{\text{new}} = 0 - 0.1 × (-4) = 0.4$

**第二次迭代**：

- 当前：$W=0.933$, $B=0.4$
- 预测值：[1.333, 2.266, 3.199]
- 误差：[-0.667, -1.734, -2.801]
- 梯度计算：
  - ${\partial W} = (-0.667×1 + -1.734×2 + -2.801×3)/3 ≈ -4.27$
  - ${\partial B} = (-0.667 + -1.734 + -2.801)/3 ≈ -1.73$
- 参数更新：
  - $W_{\text{new}} = 0.933 - 0.1 × (-4.27) = 1.360$
  - $B_{\text{new}} = 0.4 - 0.1 × (-1.73) = 0.573$

可以看到W和B正在向真实值(W=2, B=0)靠近！

## 完整训练循环

```python
def train_linear_regression(X, y, learning_rate=0.01, epochs=1000):
    """
    完整的线性回归训练过程
    """
    # 初始化参数
    W = 0
    B = 0
    m = len(X)
    
    # 记录损失历史
    losses = []
    
    for epoch in range(epochs):
        # 计算预测值
        predictions = W * X + B
        
        # 计算损失
        loss = sum((predictions - y) ** 2) / (2 * m)
        losses.append(loss)
        
        # 计算梯度
        dW = sum((predictions - y) * X) / m
        dB = sum(predictions - y) / m
        
        # 更新参数
        W = W - learning_rate * dW
        B = B - learning_rate * dB
        
        # 打印进度
        if epoch % 100 == 0:
            print(f"Epoch {epoch}: Loss = {loss:.4f}, W = {W:.4f}, B = {B:.4f}")
    
    return W, B, losses
```

## 关键要点总结

1. **更新方向**：总是往梯度**反方向**走（因为要最小化损失）
2. **步长控制**：学习率决定每一步走多大
3. **批量更新**：用所有数据点的平均梯度来更新
4. **迭代过程**：重复"计算梯度 → 更新参数"直到收敛

这样，通过不断地用梯度信息更新W和B，我们就能逐渐找到使损失最小的最优参数！
