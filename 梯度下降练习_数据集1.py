"""
æ¢¯åº¦ä¸‹é™ç»ƒä¹ _æ•°æ®é›†1.py
2025/12/29
å“‡å¹´åº•äº†è¿˜åœ¨å†™ä»£ç ï¼Œæ²¡æ•‘äº†å‘¢(æ— æ„Ÿæƒ…)
"""

# ============

# å¯¼å…¥æ‰€éœ€çš„åº“â€¦â€¦å¥½å§æˆ‘å…¶å®æ ¹æœ¬æ²¡æœ‰ç”¨åˆ°numpyæˆ–è€…sklearnä¹‹ç±»çš„ï¼Œæˆ‘å†™äº†å‡ æ¬¡æ¢¯åº¦ä¸‹é™ä¸€ä¸ªåº“éƒ½æ²¡ç”¨åˆ°(ç›®ç§»)

#============

X_train = [2.5, 1.5, 3.0, 2.0, 4.0, 3.5, 1.0, 5.0, 4.5, 6.0]    #ç‰¹å¾

y_labels = [5.0, 3.0, 6.0, 4.0, 8.0, 7.0, 2.0, 10.0, 9.0, 12.0] #æ ‡ç­¾
y_labels = [5.0, 3.0, 6.0, 4.0, 8.0, 7.0, 2.0, 10.0, 9.0, 12.0] #æ ‡ç­¾

# ============

k = 0.00    #æ–œç‡åˆå§‹åŒ–
w = 0.00    #æˆªè·åˆå§‹åŒ–
w = 0.00    #æˆªè·åˆå§‹åŒ–

def normalize_features(X):
    pass

def compute_cost(X, y, k, w):
def compute_cost(X, y, k, w):
    """
    æŸå¤±å‡½æ•°
    """
    num_samples = len(X)                    #æ ·æœ¬æ•°é‡
    total_cost = 0.0                        #æŸå¤±åˆå§‹åŒ–
    for i in range(num_samples):            #è®¡ç®—æŸå¤±
        prediction = k * X[i] + w           #é¢„æµ‹å€¼
        error = prediction - y[i]           #è¯¯å·®
        total_cost += error ** 2            #å¹³æ–¹è¯¯å·®ç´¯åŠ 
    return total_cost / (2 * num_samples)   #è¿”å›å¹³å‡æŸå¤±
    num_samples = len(X)                    #æ ·æœ¬æ•°é‡
    total_cost = 0.0                        #æŸå¤±åˆå§‹åŒ–
    for i in range(num_samples):            #è®¡ç®—æŸå¤±
        prediction = k * X[i] + w           #é¢„æµ‹å€¼
        error = prediction - y[i]           #è¯¯å·®
        total_cost += error ** 2            #å¹³æ–¹è¯¯å·®ç´¯åŠ 
    return total_cost / (2 * num_samples)   #è¿”å›å¹³å‡æŸå¤±

def compute_gradient(X, y, k, w):
def compute_gradient(X, y, k, w):
    """
    è®¡ç®—æ¢¯åº¦
    """
    num_samples = len(X)                    #æ ·æœ¬æ•°é‡
    dk = 0.00                               #æ–œç‡æ¢¯åº¦åˆå§‹åŒ–
    num_samples = len(X)                    #æ ·æœ¬æ•°é‡
    dk = 0.00                               #æ–œç‡æ¢¯åº¦åˆå§‹åŒ–
    dw = 0.00
    for i in range(num_samples):            #æ›´æ–°æµç¨‹
        prediction = k * X[i] + w           #é¢„æµ‹å€¼
        error = prediction - y[i]           #è¯¯å·®
        dk += error * X[i]                  #æ–œç‡æ¢¯åº¦
        dw += error                         #æˆªè·æ¢¯åº¦
    dk /= num_samples                       #å–å¹³å‡
    dw /= num_samples
    return dk, dw

def update_parameters(k, w, dk, dw, learning_rate):
def update_parameters(k, w, dk, dw, learning_rate):
    """
    æ›´æ–°å‚æ•°
    """
    k -= learning_rate * dk                 #æ›´æ–°æ–œç‡
    w -= learning_rate * dw                 #æ›´æ–°æˆªè·
    return k, w

def train(X, y, k, w, learning_rate=0.01, epochs=1000, 
          loss_threshold=1e-10, patience=20):
    """
    è®­ç»ƒæ¨¡å‹
    """
    prev_cost = float('inf')
    no_improve_count = 0
    
    for epoch in range(epochs):
        k_gradient, w_gradient = compute_gradient(X, y, k, w)
        k, w = update_parameters(k, w, k_gradient, w_gradient, learning_rate)
        
        cost = compute_cost(X, y, k, w)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦è¶³å¤Ÿå°
        if cost < loss_threshold:
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆäºç¬¬{epoch}æ¬¡è¿­ä»£ï¼æŸå¤±={cost}")
            print(f"æœ€ç»ˆå‚æ•°: k={k}, w={w}")
            return k, w
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦è¿˜åœ¨ä¸‹é™
        cost_change = prev_cost - cost
        if cost_change < 1e-8:  # ä¸‹é™å¾ˆå°
            no_improve_count += 1
        else:
            no_improve_count = 0
        
        if no_improve_count >= patience:
            print(f"âš ï¸ è®­ç»ƒæå‰åœæ­¢äºç¬¬{epoch}æ¬¡è¿­ä»£ï¼ŒæŸå¤±ä¸å†æ˜¾è‘—ä¸‹é™")
            print(f"æœ€ç»ˆæŸå¤±: {cost}, k={k}, w={w}")
            return k, w
        
        # æ¯100æ¬¡è¾“å‡ºä¸€æ¬¡
        if epoch % 100 == 0:
            print(f"è¿­ä»£ {epoch}: æŸå¤±={cost:.10f}, k={k:.6f}, w={w:.6f}")
        
        prev_cost = cost
    
    print(f"è®­ç»ƒå®Œæˆï¼ˆè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{epochs}ï¼‰")
    print(f"æœ€ç»ˆå‚æ•°: k={k}, w={w}")
    return k, w

    k -= learning_rate * dk                 #æ›´æ–°æ–œç‡
    w -= learning_rate * dw                 #æ›´æ–°æˆªè·
    return k, w

def train(X, y, k, w, learning_rate=0.01, epochs=1000, 
          loss_threshold=1e-10, patience=20):
    """
    è®­ç»ƒæ¨¡å‹
    """
    prev_cost = float('inf')
    no_improve_count = 0
    
    for epoch in range(epochs):
        k_gradient, w_gradient = compute_gradient(X, y, k, w)
        k, w = update_parameters(k, w, k_gradient, w_gradient, learning_rate)
        
        cost = compute_cost(X, y, k, w)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦è¶³å¤Ÿå°
        if cost < loss_threshold:
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆäºç¬¬{epoch}æ¬¡è¿­ä»£ï¼æŸå¤±={cost}")
            print(f"æœ€ç»ˆå‚æ•°: k={k}, w={w}")
            return k, w
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦è¿˜åœ¨ä¸‹é™
        cost_change = prev_cost - cost
        if cost_change < 1e-8:  # ä¸‹é™å¾ˆå°
            no_improve_count += 1
        else:
            no_improve_count = 0
        
        if no_improve_count >= patience:
            print(f"âš ï¸ è®­ç»ƒæå‰åœæ­¢äºç¬¬{epoch}æ¬¡è¿­ä»£ï¼ŒæŸå¤±ä¸å†æ˜¾è‘—ä¸‹é™")
            print(f"æœ€ç»ˆæŸå¤±: {cost}, k={k}, w={w}")
            return k, w
        
        # æ¯100æ¬¡è¾“å‡ºä¸€æ¬¡
        if epoch % 100 == 0:
            print(f"è¿­ä»£ {epoch}: æŸå¤±={cost:.10f}, k={k:.6f}, w={w:.6f}")
        
        prev_cost = cost
    
    print(f"è®­ç»ƒå®Œæˆï¼ˆè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{epochs}ï¼‰")
    print(f"æœ€ç»ˆå‚æ•°: k={k}, w={w}")
    return k, w

def main():
    print("å½“å‰ç‰¹å¾:", X_train)
    print("åˆå§‹å‚æ•°: k=", k, " w=", w)
    
    # è®­ç»ƒæ¨¡å‹
    final_k, final_w = train(
        X_train, y_labels, k, w, 
        learning_rate=0.075, 
        epochs=10000,
        loss_threshold=1e-10,
        patience=20
    )
    
    # äº¤äº’å¼é¢„æµ‹
    while True:
        user_input = input("\nè¯·è¾“å…¥ä¸€ä¸ªç‰¹å¾å€¼è¿›è¡Œé¢„æµ‹(è¾“å…¥'exit'é€€å‡º): ")
        if user_input.lower() == 'exit':
            break
        try:
            feature_value = float(user_input)
            prediction = final_k * feature_value + final_w
            print(f"é¢„æµ‹ç»“æœ: {prediction:.2f}")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ–'exit'é€€å‡ºã€‚")
    
    print("ç¨‹åºç»“æŸ")
    print("å½“å‰ç‰¹å¾:", X_train)
    print("åˆå§‹å‚æ•°: k=", k, " w=", w)
    
    # è®­ç»ƒæ¨¡å‹
    final_k, final_w = train(
        X_train, y_labels, k, w, 
        learning_rate=0.075, 
        epochs=10000,
        loss_threshold=1e-10,
        patience=20
    )
    
    # äº¤äº’å¼é¢„æµ‹
    while True:
        user_input = input("\nè¯·è¾“å…¥ä¸€ä¸ªç‰¹å¾å€¼è¿›è¡Œé¢„æµ‹(è¾“å…¥'exit'é€€å‡º): ")
        if user_input.lower() == 'exit':
            break
        try:
            feature_value = float(user_input)
            prediction = final_k * feature_value + final_w
            print(f"é¢„æµ‹ç»“æœ: {prediction:.2f}")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ–'exit'é€€å‡ºã€‚")
    
    print("ç¨‹åºç»“æŸ")
if __name__ == "__main__":
    main()